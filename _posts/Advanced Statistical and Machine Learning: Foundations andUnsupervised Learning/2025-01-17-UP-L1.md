---
layout: post
title: UP-L1
author: wichai
date: 2025-01-17 13:39
categories:
  - Study
  - Master
tags:
  - DU
  - UL
  - ASML
mermaid: true
math: true
pin: false
---

# Part 1: Basics of multivariate analysis

## First: What is learning?
- Learning means finding <u>probability distributions</u> to model data *$Y$*.
	- 概率分布对数据建模
- We will see many examples of this **Foundations**:
	- Estimating the parameters $a$ of a parametric model $P(Y|a)$.
- So we will know how to learning.

## Supervised Learning 监督式学习
- Special case of learning.
- 'Supervised' means that data comes **in pairs**: 成对
	- In addition to $Y$ there are conditioning variables $X$.
	- The task is to $P(Y|X)$.
- Essentially the same framework as 'conditional models' from Foundations, so we will know how to do that too.
- What about 'unsupervised'?

## Unsupervised Learning 无监督学习
- Special case of learning. (Again)
- 'Unsupervised' means one of two things:
	- The distribution does not depend on $X$.
		- 分布不依赖 X
	- We simply do not know the corresponding $X$ values.
		- X值是未知的
- Case 1 seems straightforward: there is no $X$.
![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250117135325.png)
- Case 2 seems impossible: $X$ could be anything!
![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250123134649.png)

- Both these things are true, so why do we have this submodule?!

## Plotting k-means output

```python
plot(intro.dat, col=k3$cluster, main="k-means")

points(k3$centwes, pch=15, cex=2, col=c(1, 2, 3))
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250117141126.png)

## Dissmilarity 差异性

To understand on a deeper level what $k$-means is trying to achieve, one needs to introduce the concept of a **dissimilarity**, $D(x_i,x_i')$ between two points $x_i$ and $\boldsymbol{x}_i^{\prime}\in\mathbb{R}^p$.
Often, these dissimilarities are defined component-wise (按组件定义), based on dissimilarity measures between the the values of the $j$th variable (attribute),

$$
d_j(x_{ij},x_{i^{\prime}j})=\ell(x_{ij}-x_{i^{\prime}j})
$$

with a loss function $l(\cdot)$; for instance,
- $\ell(z)=z^2$ (squared error loss);
- $\ell(z)=|z|$ (absolute loss).

The $p$ attribute-wise dissimilarities can be merged into a single overall measure of dissimilarity between objects $i$ and $i'$ via

$$
D(\boldsymbol{x}_i,\boldsymbol{x}_i^{\prime})=\sum_{j=1}^pw_jd_j(x_{ij},x_{i^{\prime}j})
$$

where $w_j$ is a set of weights.
Simple example: Set $l(z)=z^2$ and $w_j=(1,...,1)$. Then

$$
D(\boldsymbol{x}_i,\boldsymbol{x}_i^{\prime})=\sum_{i=1}^n(x_{ij}-x_{i^{\prime}j})^2=||\boldsymbol{x}_i-\boldsymbol{x}_i^{\prime}||^2=(\boldsymbol{x}_i-\boldsymbol{x}_i^{\prime})^T(\boldsymbol{x}_i-\boldsymbol{x}_i^{\prime}).
$$

(squared Euclidean distance)

## Dissmilarity and k-means
Note firstly that, for a collection of data $x_1, . . . , x_n$, one can describe the iterative part of the k-means algorithm entirely through two optimization steps rooted in dissimilarities_:
(i) For a given set of cluster centres, say $c_k$, $k = 1 . . . ,K$, assign each observation $x_i$to partition $C_k$, with k given by 

$$
k=\operatorname{argmin}_\ell D(\boldsymbol{x}_i,\boldsymbol{c}_\ell).
$$

(ii) For a given partition $C_1, . . . ,C_K$, find the updated centre via

$$
c_k^{\prime}=\mathrm{Mean}(\boldsymbol{x}_i|\boldsymbol{x}_i\in C_k)=\mathrm{argmin}_m\sum_{\boldsymbol{x}_i\in C_k}D(\boldsymbol{x}_i,\boldsymbol{m})
$$

If $D$ is the squared Euclidean distance, one can show that the $k$-means algorithm finds a minimum of the within-cluster sum of squares
where $n_k$ _is the number of observations in partition $k$.
This value can be compared to the  total sum of squares.

## Decomposing variation

```
k3$withinss

## [1] 270.39234 89.49661 258.54463

k3$tot.withinss  # SS_within

## [1] 618.4336

k3$totss  # SS_total

## [1] 2681.893

k3$betweenss  # SS_between

## [1] 2063.459
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250117154954.png)

**Sum of squares:** 2681.893
![](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250117154954.png)

**Sum of squares:** 618.4336

## Analysis in R: Visualize projections
**Total**

```R
m <- colMeans(intro.dat)
plot(intro.dat,
     main="distances to
     overall mean")
points(m[1] , m[2], pch=15,
       cex=2, col=1)
n<-dim(intro.dat)[1]
for (j in 1:n){
  segments(intro.dat[j,1],
           intro.dat[j,2],
           m[1], m[2], col=4)
}
```

**Cluster-wise**

```R
plot(intro.dat, col=k3$cluster,
     main="distances to
     cluster centres")
points(k3$centers, pch=15,
       cex=2, col=c(1,2,3))
       
for (j in 1:n){
  segments(intro.dat[j,1],
           intro.dat[j,2],
           k3$centers[k3$cluster[j],1],
           k3$centers[k3$cluster[j],2],
           col=k3$cluster[j])
}
```

## k-Means, chickens, and eggs
Note that k-means is a chicken-and-egg problem….

- Determining the partitions requires knowing the means
- Finding the means requires knowing the partitions
- How to get started is a rather arbitrary decision!

## k-Means: not a unique technique

There do exist many similar k-means algorithms, some of which differ in how this initialization step is handled, and some in other aspects:

The algorithm provided on slide 12 is Lloyd-Forgy’s algorithm (Lloyd 1957, published 1982; independently by Forgy, 1965).

Alternative: Random partitioning: First, randomly assign a cluster label to each observation, and then find the mean of these clusters. (Excellent illustration in [J Sec 10.3.1]).

Default option in R: Hartigan-Wong algorithm. Initialization as in random partitioning, but then steps (i) and (ii) are iterated point-by-point rather than for the whole data set at once [H Sec 14.3.6.].

Whether any of these variants find the global minimum of the total within-cluster sum of squares, will depend on the seed (Try!):

```R
k31<- kmeans(intro.dat, centers=3, algorithm="Forgy")
k31$centers

##      x          y
## 1 -0.8169763  2.996031
## 2  0.2621725 -1.267684
## 3  2.1752499  3.826271

k31$tot.withinss
## [1] 618.4336

k3$iter
## [1] 8

k3 <- kmeans(intro.dat, centers=3, algorithm="Hartigan-Wong")
k3$centers
##      x          y
## 1  0.2621725 -1.267684
## 2 -0.8169763  2.996031
## 3  2.1752499  3.826271

k3$tot.withinss

## [1] 618.4336

k3$iter

## [1] 3
```

As with any algorithm that finds a local minimum, one can increase the chances of finding the global minimum for any variant by using multiple random starts; see option `nstart` in `kmeans`.

## k-Medoids

There are occasions when it is desirable that the cluster centres corresponds to actual observations (‘the most central observation of the cluster’).

Recall: In ‘usual’ k-means, in step (ii), finding the $k$th cluster mean is equivalent to minimizing $\sum_{\boldsymbol{x}_{i}\in C_{k}}D(\boldsymbol{x}_{i},\boldsymbol{m})$ over all possible centres $m$. k-Medoids replaces this by an optimization step which restricts the search space to the observations within the $k$-th cluster; i.e.

$$
c_{k}^{\prime}=\mathrm{argmin}_{m\in C_{k}}\sum_{x_{i}\in C_{k}}D(\boldsymbol{x}_{i},\boldsymbol{m}).
$$

## Example: Where’s Wally?
![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250118014226.png)

Data gives the locations ($X,Y$ coordinates) of where Wally (called Waldo in the US) was found in $n = 68$ double-paged illustrations in a total of seven ‘Where’s Wally’ books.

```R
wally.pts = read.csv('http://www.randalolson.com/wp-content/uploads/wheres-waldo-locations.csv')

plot(wally.pts$X, wally.pts$Y)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250118014351.png)

```R
library(clue)
wally.kmed<- kmedoids(dist(wally.pts[,c("X","Y")]), k=3)
plot(wally.pts$X, wally.pts$Y)
points(wally.pts$X[wally.kmed$medoid_ids],
       wally.pts$Y[wally.kmed$medoid_ids],
       col=2, pch=9, cex=1.8)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250118014412.png)
Compare to $k$-means:

```R
wally.kmean <- kmeans(wally.pts[,c("X","Y")], centers=3)
points(wally.kmean$centers, col=3, pch=10, cex=2)
legend(2, 8, legend=c("k-medoids", "k-means"), pch=c(9,10), col=c(2,3)) #$
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250118014508.png)

## Data matrix
Let $X$ denote a data matrix (or data frame) with $n$ rows and $p$ columns (variables, features); i.e.

$$

$$

For instance, the data on slide 3 (right plot) constitute such a matrix with $( n = 320 )$ and $( p = 2 )$:

```R
dim(intro.dat)

## [1] 320 2
```

##  Random vectors
We can combine the $p$ components of each row of $X$ into a vector  $X$ in $mathbb{R}^p$. The rows of $X$ can then be thought of as realizations of a random vector, with the $j$-th component, $X_j$, being responsible for the generation of the $j$-th column of $X$.

We have implicitly been using the properties of random vectors in $\mathbb{R}^2$ so far. We now make these properties more explicit and define some quantities for future use.

## Properties of random vectors

(a) There exists (under some mild conditions), a probability density function (‘density’) $f : \mathbb{R}^p \to \mathbb{R}_{0}^+$, such that

$$
P(X \in dx) = dx f(x) \tag{1}
$$

$$
P(X \in S) = \int_S dx f(x) = \int_S f(x) dx \tag{2}
$$

for any (appropriately well-behaved) subset $S \subset \mathbb{R}^p$. In particular, for $S = \mathbb{R}^p, \int_{\mathbb{R}^p} f(x) dx = 1$.
The density $(f)$ can be **estimated** from data $(X)$; we deal with this problem in Part 2 of this course.

The density $(f)$ can be estimated from data $(X)$; we deal with this problem in Part 2 of this course.

(b) We refer to the $X_1, \ldots, X_p$ as independent if the joint density $f(x)$ can be factorized; i.e. if

$$

f(x_1, \ldots, x_p) = f(x_1) \times \cdots \times f(x_p).

$$

  This means that the values of any random variable $X_j$ are uninformative for the values of the other random variables.
  
(c) The random vector $X$ has a mean:

$$
\begin{pmatrix}
m_1 \\
\vdots \\
m_p
\end{pmatrix}
= m = E(X) = \int x f(x) dx =
\int \cdots \int \begin{pmatrix}
x_1 \\
\vdots \\
x_p
\end{pmatrix} f(x_1, \ldots, x_p) dx_1 \cdots dx_p
$$

which implies $m_j = E(X_j)$, for the $j$-th component of $m$.

## Estimating the mean vector

Let $\mathbf{x}_i^T$ denote the $i$-th row of $\mathbf{X}$, i.e., the $i$-th observation. Then we may estimate $\mathbf{m}$ by

$$
\bar{\mathbf{x}} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i = \frac{1}{n} \sum_{i=1}^{n} 
\begin{pmatrix}
x_{i1} \\
\vdots \\
x_{ip}
\end{pmatrix} = 
\begin{pmatrix}
\frac{1}{n} \sum_{i=1}^{n} x_{i1} \\
\vdots \\
\frac{1}{n} \sum_{i=1}^{n} x_{ip}
\end{pmatrix} = 
\begin{pmatrix}
\bar{x}_1 \\
\vdots \\
\bar{x}_p
\end{pmatrix}
$$

One can easily show that this estimator is unbiased: if we were able to draw repeated samples of size $n$ from $\mathbf{X}$, and then compute $\bar{\mathbf{x}}$ each time, the average of these estimates would tend to the true value of ${m}$ as the number of samples increased.

### Estimating the mean vector: Example

```r
m <- colMeans(intro.dat)
plot(intro.dat)
points(m[1], m[2], col=2, pch=15, cex=2)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119152822.png)

## Properties of random vectors (cont’d)

(d) The random vector also possesses a variance (also called variance matrix, covariance matrix),

$$ 
Var(X) = E((X - m)(X - m)^T) = E(XX^T) - mm^T 
$$

$$
= \begin{pmatrix}
\Sigma_{11} & \cdots & \Sigma_{1p} \\
\vdots & \ddots & \vdots \\
\Sigma_{p1} & \cdots & \Sigma_{pp}
\end{pmatrix} = \Sigma,
$$

where

$$
\Sigma_{ij} = Cov(X_i, X_j) = E(X_iX_j) - E(X_i)E(X_j) \quad i \neq j 
$$

$$
\Sigma_{jj} \equiv \sigma_j^2 = Var(X_j)
$$

Any variance matrix $\Sigma$ has the following properties:

(i) $\Sigma$ is symmetric, i.e. $\Sigma = \Sigma^T$  
(ii) $\Sigma$ is positive semi-definite; i.e. its eigenvalues are non-negative.

In short, we write  

$$ X \sim (m, \Sigma) $$

meaning that $X$ has some unspecified distribution with mean $m$ and variance $\Sigma$.

## Estimating variance matrices

Firstly, recall 

$$
\Sigma = \text{Var}(X) = E \left( (X - m)(X - m)^T \right).
$$

Replacing all expectations by means, a natural candidate estimator for $\Sigma$ is given by 

$$
\hat{\Sigma} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T = \frac{1}{n} \sum_{i=1}^{n} x_ix_i^T - \bar{x}\bar{x}^T \in \mathbb{R}^{p \times p}.
$$

In fact, this is the Maximum Likelihood estimator for $\Sigma$ under an assumption of multivariate normality, but it is biased.

An unbiased estimator of $\Sigma$ is obtained through the sample variance matrix,

$$
\hat{\Sigma} = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T = \frac{n}{n - 1} \tilde{\Sigma}
$$

We generally prefer $\hat{\Sigma}$ to $\tilde{\Sigma}$. R functions var and cov (identical!) use this too.

In the special case of $p = 2$, $\hat{\Sigma}$ can be written as

$$
\hat{\Sigma} = \frac{1}{n - 1} \begin{pmatrix}
\sum_{i=1}^{n} (x_{1i} - \bar{x}_{1})^2 & \sum_{i=1}^{n} (x_{1i} - \bar{x}_{1})(x_{2i} - \bar{x}_{2}) \\
\sum_{i=1}^{n} (x_{1i} - \bar{x}_{1})(x_{2i} - \bar{x}_{2}) & \sum_{i=1}^{n} (x_{2i} - \bar{x}_{2})^2
\end{pmatrix}
$$

### Estimating variance matrices: Example

```r
Sigma <- var(intro.dat)
Sigma

##         x         y
## x 2.439586 1.205168
## y 1.205168 5.967602
```

## # Correlation matrix

The correlation matrix is defined as 

$$
\mathcal{R} = (R_{ij})_{1 \leq i \leq p, 1 \leq j \leq p}
$$

with pairwise correlation coefficients

$$
R_{ij} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii} \Sigma_{jj}}}
$$

The matrix $\mathcal{R}$ is scale-invariant, and all diagonal elements are equal to 1. We will use this matrix mainly for PCA (Part 4).

We call the random variables $X_i$ and $X_j$ uncorrelated if $R_{ij} = 0$. Note:

- If all $X_i, i = 1, \ldots, p$ are uncorrelated then $\mathcal{R}$ becomes the identity matrix;
- If $X_i$ and $X_j$ are independent then they are uncorrelated (but the converse does not hold).

### Estimating correlation matrices: Example

```r
R <- cor(intro.dat)
R

##        x          y
## x 1.0000000 0.3158564
## y 0.3158564 1.0000000
```

## The normal distribution

A random mechanism tends to have approximately a normal (aka Gaussian) distribution if its deviation from the average is the cumulative result of many independent influences.

### Examples:
- Measurement errors;
- Minor variations in production of things (e.g. thickness of coins);
- Distribution of marks in a test (of an equally skilled cohort);
- Heights of people sampled from a population.

We will see this and other ways of justifying a Gaussian distribution in Foundations.

A famous tool to mimic a normal distribution is the Galton Board:

```R
require(animation)
set.seed(42)
balls <- 200
layers <- 15
ani.options(nmax = balls + layers - 2, 2)
galton.sim = quincunx(balls = balls,
                       col.balls = rainbow(layers))
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119153634.png)
Galton Board in progress.

See full animation in R!
Final outcome of animation:

```R
barplot(galton.sim, space = 0)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119153722.png)

Probability density function of a (univariate) normal distribution with mean $\mu$ and variance $\sigma^2$, short $X \sim N(\mu, \sigma^2)$:

$$
f(x | \mu, \sigma^2) = \phi(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
$$

For instance, density function for $\mu = 5$ and $\sigma^2 = 3$:

```r
x <- seq(-2, 12, length=101)
plot(x, dnorm(x, mean=5, sd=sqrt(3)), type="l", lwd=2, col=2)
abline(v=5, col=3)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119154115.png)

> “[The normal distribution] cannot be obtained by rigorous deductions. Several of its putative proofs are awful [. . .]. Nonetheless, everyone believes it, as M. Lippmann told me one day, because experimenters imagine it to be a mathematical theorem, while mathematicians imagine it to be an experimental fact.”
> — Henri Poincaré, Le calcul des Probabilités. 1896

Actually not true! Maximum entropy and the central limit theorem justify the Gaussian distribution.

## Multivariate normal distribution

A random vector $X = (X_1, X_2, \ldots, X_p)^T$ is multivariate normal if (and only if) any linear combination of the random variables $X_1, X_2, \ldots, X_p$ is univariate normally distributed, i.e. iff

$$
a_1X_1 + a_2X_2 + \cdots + a_pX_p
$$

has a univariate normal distribution for any constants $a_1, a_2, \ldots, a_p$.

- Setting $a_j = 1$, and all others $a_\ell = 0, \ell \neq j$, we see that multivariate normality of $X$ implies univariate normality of each $X_j$ (the reverse is not true)!

- The definition includes the limiting case that $(a_1, \ldots, a_p) \equiv 0$, in which case $X$ is a $p$-variate point mass at 0 (a ‘Dirac delta function’, and strictly speaking no longer a density).

When a positive-definite variance matrix $\Sigma$ exists, then the density of a multivariate normal (MVN) distribution takes the form

$$
\phi(x|\mu, \Sigma) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp \left\{ -\frac{1}{2}(x - \mu)^{T} \Sigma^{-1} (x - \mu) \right\} , \quad (3)
$$

with parameters $\mu \in \mathbb{R}^p, \Sigma \in \mathbb{R}^{p \times p}$. We write then 

$$
X \sim N_p(\mu, \Sigma).
$$

Example for MVN density: 

$$
\mu = \begin{pmatrix} 5 \\ 5 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 3^2 & 0 \\ 0 & 2^2 \end{pmatrix}
$$

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119154538.png)

### R Code for displaying density

```r
# creates an appropriate grid
x1 <- seq(-5,15, length=51)  # 51 is an arbitrary grid size
x2 <- seq(-5,15, length=51)
dens <- matrix(0,51,51)

# defines mu and Sigma
mu <- c(5,5)
Sigma <- matrix(c(9,0,0,4), byrow=TRUE, ncol=2)

# fills grid with density values
require(mvtnorm)
for (i in 1:51) {
  for (j in 1:51) {
    dens[i,j] <- dmvnorm(x=c(x1[i],x2[j]), mean=mu, sigma=Sigma)
  }
}

persp(x1, x2, dens, theta=40, phi=20)  # draws the density in 3D
contour(x1, x2, dens)  # draws contour plots in 2D
```

Some observations (based on the example from the last two slides):

- From the marginalization property, we can conclude that $X_1 \sim N(5, 3^2)$ and $X_2 \sim N(5, 2^2)$;
- For the matrix $\Sigma$ used in the last two slides, all entries off the diagonal are zero. Statistically, the entries off the diagonal are the covariances between the two random variables $X_1$ and $X_2$. If these are 0, then $X_1$ and $X_2$ are uncorrelated. If $X$ is MVN with uncorrelated components $X_1$ and $X_2$, then these are also independent, i.e. $f(x_1, x_2) = f(x_1) \times f(x_2)$.

What happens if the two components are not uncorrelated? Assume now $Cov(X_1,X_2) = 3$, that is 

$$
\Sigma = 
\begin{pmatrix}
3^2 & 3 \\
3 & 2^2
\end{pmatrix}
$$

The only actual difference to the previous code is 

```R
Sigma <- matrix(c(9,3,3,4), byrow=TRUE, ncol=2)
```

but for aesthetic reasons we slightly change the graphical parameters:

```R
persp(x1, x2, dens, theta=40, phi=20)
contour(x1, x2, dens, nlevels=20)
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119155214.png)

## Quick reminder: Variance matrix needs to be valid

Can we choose ‘anything’ for Σ?

No! It needs to be symmetric and positive definite or the density cannot be normalized. Check the latter via

```r
eigen(Sigma)$values
## [1] 10.405125 2.594875
```

$\Sigma$ is positive definite if and only if all eigenvalues are positive.

## Weighted Dissimilarities

Sometimes attributes operate on very different scales (for instance, kg, meters, etc.), which could distort the dissimilarities. This can be mitigated by defining appropriate weights. Common choices are 

(i) $w_j = \frac{1}{\bar{d}_j}$, where $\bar{d}_j$ is the mean over all $n^2$ pairwise dissimilarities over all $n$ observations (gives all attributes equal influence) [H1 Sec 14.3.3].

(ii) $w_j = \frac{1}{\sigma_j^2}$, where $\sigma_j$ is the (known or estimated) standard deviation of the $j$th attribute (scales all attributes to unit variance).

## Mahalanobis distances

The dissimilarities created in (ii) can be written as

$$ D(x_i, x_i') = \sum_{j=1}^{p} \frac{1}{\sigma_j^2}(x_{ij} - x_{ij'})^2 = (x_i - x_i')^T \begin{pmatrix}
\sigma_1^{-2}  \\
 & \ddots &  \\
 & & \sigma_p^{-2}
\end{pmatrix} (x_i - x_i') $$

This can be generalized by replacing the diagonal matrix by the inverse of a full variance matrix $\Sigma$ (again, known or estimated), yielding what is known as the (squared) Mahalanobis distance:

$$ D_M(x_i, x_i') = (x_i - x_i')^T \Sigma^{-1}(x_i - x_i') $$

Have we seen this dissimilarity before? Yes, in MVN density!

Note that we can write the $N(\mu, \Sigma)$ density function as

$$

f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left\{ -\frac{1}{2} D(x, \mu) \right\},

$$

therefore points with equal Mahalanobis distance to the mean lie on the same contour of the respective Gaussian distribution.

An interesting property of Mahalanobis distances to the mean is that, when considered as a random variable,

$$ 

D_M(X, \mu) \approx \chi^2(p) 

$$

where $\chi^2(p)$ denotes a $\chi^2$ distribution with $p$ degrees of freedom.

This result can be used for **outlier detection** and **testing multivariate normality**.

## Outlier detection

For instance, for Wally’s data, compute all squared Mahalanobis distances and compare with the 2.5% tail quantile of χ²(2):

```R
wally.m <- colMeans(wally.pts[,c("X","Y")])
wally.S <- var(wally.pts[,c("X","Y")])
wally.Mdist <- mahalanobis(wally.pts[,c("X","Y")], 
                            center=wally.m, cov=wally.S)
detect <- which(wally.Mdist > qchisq(0.975,2))
detect

## [1] 1
```

```R
plot(wally.pts$X, wally.pts$Y)
points(wally.m[1], wally.m[2], pch=15, cex=2, col=2)
points(wally.pts$X[detect], wally.pts$Y[detect], pch=16, col=3, cex=2)
legend(2,8, pch=c(15,16), col=c(2,3), legend=c("mean", "identified outlier"))
```

![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119160015.png)

## Checking multivariate normality

Compare to data generated from a MVN:

```R
require(mvtnorm)
mu <- c(5,5)
Sigma <- matrix(c(9,0,0,4), byrow=TRUE, ncol=2)
Z <- rmvnorm(300, mean=mu, sigma=Sigma)
Z.Mdist <- mahalanobis(Z, mu, Sigma)
qqplot(qchisq(ppoints(300), df=2), Z.Mdist)
abline(a=0,b=1, col=2, lwd=2)
text(3,10,"clearly multivariate normal")
```
![image.png](https://wichaiblog-1316355194.cos.ap-hongkong.myqcloud.com/20250119160048.png)

## # Practical 1

In the first lab session (fetch Practical 1 on Jupyter), we will:

- carry out some simple exploratory data analysis of a four-dimensional, oceanographic data set;
- apply k-means to this data set; identify the cluster centres and visualize the clusters;
- experiment with different settings of k-means and different numbers of clusters;
- search for outliers and check for multivariate normality of this data set.
