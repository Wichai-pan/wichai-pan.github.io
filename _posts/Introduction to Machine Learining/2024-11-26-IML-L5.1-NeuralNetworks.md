---
layout: post
title: IML L5.1 Neural networks
author: wichai
date: 2024-11-26 09:00 +0000 
categories: [Study, Master]
tags: [DU, AI, ML]
mermaid: true
math: true
pin: false
---



# Neural networks

Neutral networks started as an attempt to model the human brain. It is composed of neurones

- Consist of layers of interconnected nodes called **neurons** .
- Each neuron processes input and passes the output to the next layer.
- Capable of learning complex patterns from data.
- Used in various domains like computer vision, natural language processing, and more.

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/neuron.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/neuron.png)

The model for a neuron was the perceptron:

Models a single neuron.
模拟单个神经元。

Performs binary classification.
执行二元分类。

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/perceptron.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/perceptron.png)

Mathematically, the perceptron computes: 感知器计算

$$
z = w_0 + \vec w \cdot \vec x; \ \ \ \ \ output = \phi(z)
$$

$\vec x = $ input vector 输入向量

$\vec w=$ weight vector 权重向量

$w_0=$ bias term 偏置项

$\phi(z)=$ activate function 激活函数

This is the model of a modern unit in a neural network:

- linear combination of inputs units
- non-linear function to generate the unit’s value, called *activation function*

The perceptron learning rule updates weights based on the error:
感知器学习规则根据错误更新权重：
$$
w_j \leftarrow w_j + \eta(y-\hat y)x_j
$$
where: 在哪里:

- $\eta$ = learning rate. 学习率。
- $y$ = true label.
- $\hat y$ = predicted label.



## Limitations of the Perceptron 感知器的局限性

The perceptron has significant limitations:
感知器有显著的局限性：

- Can only solve linearly separable problems.
  只能解决线性可分问题。
- Cannot model complex, non-linear decision boundaries (e.g., XOR problem).
  无法建模复杂的非线性决策边界（例如，异或问题）。
- Limited representational capacity.
  有限的表征能力。
- Led to the development of multi-layer networks to overcome these limitations.
  导致了多层网络的发展以克服这些限制。



## Non-linear activation functions

There are different decision functions that can be used.

- the perceptron used a step function
- logistic regression uses the sigmoid function
- one can also use $\phi(z) = tanh(z)$ as a decision function
- various variations on the hinge function (ReLU)

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/decision.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/decision.png)



## Network architectures

Neural networks are build by connecting artificial neurons together.

They can be used for classification or regression. Here we will consider the classification case.

## Single layer network

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/onelayer.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/onelayer.png)

## Multi layer network

Also known as Multi-Layer Perceptrons (MLPs).
也被称为多层感知器（MLPs）。

- Consist of an input layer, one or more hidden layers, and an output layer.
  由一个输入层、一个或多个隐藏层和一个输出层组成。
- Hidden layers enable the network to learn complex, non-linear patterns.
  隐藏层使网络能够学习复杂的非线性模式。
- Universal approximation theorem: MLPs can approximate any continuous function.
  通用逼近定理：MLP 可以逼近任何连续函数。

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/multilayer.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/multilayer.png)

Networks with a large number of layers are referred to as “deep learning”.

They are more difficult to train, recent advances in training algorithms and the use of GPUs have made them much tractable (and popular!).



## Deep Learning 深度学习

Networks with a large number of layers are referred to as **deep learning** .
具有大量层的网络被称为深度学习。

Advantages: 优势:

- Can learn hierarchical representations.
  可以学习分层表示。
- Effective in processing high-dimensional data like images, speech, and text.
  处理高维数据（如图像、语音和文本）方面非常有效。



Challenges: 挑战:

- Require large amounts of data.
  需要大量数据。
- Computationally intensive training.
  计算密集型训练。
- Potential for overfitting.
  过拟合的潜在风险。



Advances in hardware (GPUs, TPUs) and algorithms (e.g., optimization techniques) have made deep learning feasible.
硬件（GPU、TPU）和算法（例如，优化技术）的进步使得深度学习成为可能。



## Deep Learning and Hierarchical Feature Learning 深度学习和分层特征学习

Deep learning leverages multiple layers to learn hierarchical representations:
深度学习利用多层来学习分层表示：

- **Lower Layers:** Capture simple features like edges or textures.
  底层：捕捉简单特征，如边缘或纹理。
- **Middle Layers:** Combine simple features to form more complex patterns.
  中间层：将简单特征组合成更复杂的模式。
- **Higher Layers:** Abstract high-level concepts relevant to the task.
  Higher Layers: 与任务相关的抽象高层概念。

This hierarchy enables neural networks to automatically learn features from raw data.
这种层次结构使神经网络能够自动从原始数据中学习特征。



## Deep Learning and Hierarchical Feature Learning 深度学习和分层特征学习

Deep learning leverages multiple layers to learn hierarchical representations:
深度学习利用多层来学习分层表示：

- **Lower Layers:** Capture simple features like edges or textures.
  底层：捕捉简单特征，如边缘或纹理。
- **Middle Layers:** Combine simple features to form more complex patterns.
  中间层：将简单特征组合成更复杂的模式。
- **Higher Layers:** Abstract high-level concepts relevant to the task.
  Higher Layers: 与任务相关的抽象高层概念。



This hierarchy enables neural networks to automatically learn features from raw data.
这种层次结构使神经网络能够自动从原始数据中学习特征。

![img](https://miscada-ml-2425.notes.dmaitre.phyip3.dur.ac.uk/assets/presentations/lecture-5/hierarchical_feature_learning.png)







## Feedforward step

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/input_output.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/input_output.png)

To calculate the output of the $j$-th unit for the $i$-th data example we calculate


$$
{o_j^{(i)}=\phi(z_j^{(i)});,\quad z_j^{(i)}=w_{0j}+\sum_{k=1}^{n_i}w_{kj}\cdot x_k^{(i)}}
$$


If we have a $n_i$ input nodes and $n_0$ output units we have $(n_i +1)\times n_0$ parameters $w_{kj}$. We can organise them in a $n_i+1$ by $n_0$ matrix of parameters $W$ and write the vector of $x$ values for the $n_0$ output units as


$$
z = W^Tx
$$


where xx is the column vector of input values with a $1$ added as the $0$-th component.

The value $z$ is then passed through the non-linear function.

## Last step

If we have more that 2 classes we want the last layer’s output to be probabilities of belonging to one the the $k$ classes in the classifier. For this we replace the sigmoid function with the *softmax* function:


$$
{s_{i}(z)=\frac{e^{z_{i}}}{\Sigma_{j=1}^{k}e^{z_{j}}}}
$$


It is by definition normalised such that the sum adds to one and is the multi-class generalisation of the sigmoid function. $s_i$ is largest for the index $i$ with the largest $z_i$. The classifier prediction will typically be the class $i$ with the largest value $s_i$.


$$
\frac{\partial s_i}{\partial z_{j}}=\frac{-e^{z_{i}}e^{z_j}+\delta_{ij}e^{z_{i}}\Sigma_{i=1}^ke^{z_i}}{\left(\Sigma_{j=1}^ke^{z_j}\right)^{2}}={s}\frac{-e^{z_j}+\delta_{ij}\Sigma_{i=1}^{\mathrm{k}}e^{z_i}}{\Sigma_{j=1}^{k}e^{z_j}}={s}\left(\delta_{ij}-s_j\right)
$$


## Multi-class loss function

The generalistaion of the cross entropy loss for multiple class is given by


$$
{J=-\sum_{i,j}y_{j}^{(i)}log(\hat y_{j}^{(i)})}
$$


where $y^{(i)}_j$ is $1$ if training sample ii is in class $j$, 0 otherwise.

This is called *one-hot encoding*. The derivative of the loss function with respect to one of the predictions is given by


$$
\frac{\partial J}{\partial \hat y_j^{(i)}}=-\frac{y_j^{(i)}}{\hat y_j^{(i)}}
$$



## Example

Using the same circle dataset that we used before, the decision function for a single-layer neural network with 10 hidden units gives the following decision boundary:

- Using a neural network to classify points in a circular pattern:
  使用神经网络来对圆形模式中的点进行分类：
  - Data is not linearly separable.
    数据不是线性可分的。
  - Single-layer networks fail to classify correctly.
    单层网络无法正确分类。
  - Multi-layer networks with hidden units can capture the circular pattern.
    具有隐藏单元的多层网络可以捕捉到循环模式。

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/circles_ML.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/circles_ML.png)

Here is an example using more than two classes.

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_3class.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_3class.png)



## Iris Dataset 鸢尾花数据集

Classifying iris flowers into three species using a neural network:
将鸢尾花分为三个物种，使用神经网络：

- Features: sepal length, sepal width, petal length, petal width.
  特征：萼片长度，萼片宽度，花瓣长度，花瓣宽度。
- Three classes: Setosa, Versicolor, Virginica.
  三个类别：Setosa，Versicolor，Virginica。
- Multi-class classification using softmax output layer.
  多类分类使用 softmax 输出层。


[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_ML.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_ML.png)



## Training Neural Networks 训练神经网络

Training involves adjusting weights to minimize a loss function:
训练涉及调整权重以最小化损失函数：

1. Initialize weights randomly or using specific initialization methods.
   随机初始化权重或使用特定的初始化方法。
2. Perform a forward pass to compute the output.
   执行前向传播以计算输出。
3. Calculate the loss using a suitable loss function.
   使用合适的损失函数计算损失。
4. Compute gradients via backpropagation.
   通过反向传播计算梯度。
5. Update weights using an optimization algorithm.
   使用优化算法更新权重。
6. Repeat the process for multiple epochs.
   重复该过程多个时代。



Key concepts: 关键概念:

- **Epoch:** One complete pass through the training dataset.
  Epoch: 一次完整的训练数据集遍历。
- **Batch Size:** Number of samples processed before updating weights.
  批量大小：在更新权重之前处理的样本数量。
- **Learning Rate:** Controls the step size during weight updates.
  学习率：控制权重更新过程中的步长。



