---
layout: post
title: IML L5.1 Neural networks
author: wichai
date: 2024-11-26 09:00 +0000 
categories: [Study, Master]
tags: [DU, AI, ML]
mermaid: true
math: true
pin: false
---



# Neural networks

Neutral networks started as an attempt to model the human brain. It is composed of neurones

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/neuron.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/neuron.png)

The model for a neuron was the perceptron:

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/perceptron.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/perceptron.png)


$$
z = w_0 + \vec w \cdot \vec x; \ \ \ \ \ outtput = \phi(z)
$$


This is the model of a modern unit in a neural network:

- linear combination of inputs units
- non-linear function to generate the unit’s value, called *activation function*

## Non-linear activation functions

There are different decision functions that can be used.

- the perceptron used a step function
- logistic regression uses the sigmoid function
- one can also use $\phi(z) = tanh(z)$ as a decision function
- various variations on the hinge function (ReLU)

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/decision.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/decision.png)

## Network architectures

Neural networks are build by connecting artificial neurons together.

They can be used for classification or regression. Here we will consider the classification case.

## Single layer network

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/onelayer.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/onelayer.png)

## Multi layer network

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/multilayer.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/multilayer.png)

Networks with a large number of layers are referred to as “deep learning”.

They are more difficult to train, recent advances in training algorithms and the use of GPUs have made them much tractable (and popular!).

## Feedforward step

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/input_output.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/input_output.png)

To calculate the output of the $j$-th unit for the $i$-th data example we calculate


$$
{o_j^{(i)}=\phi(z_j^{(i)});,\quad z_j^{(i)}=w_{0j}+\sum_{k=1}^{n_i}w_{kj}\cdot x_k^{(i)}}
$$


If we have a $n_i$ input nodes and $n_0$ output units we have $(n_i +1)\times n_0$ parameters $w_{kj}$. We can organise them in a $n_i+1$ by $n_0$ matrix of parameters $W$ and write the vector of $x$ values for the $n_0$ output units as


$$
z = W^Tx
$$


where xx is the column vector of input values with a $1$ added as the $0$-th component.

The value $z$ is then passed through the non-linear function.

## Last step

If we have more that 2 classes we want the last layer’s output to be probabilities of belonging to one the the $k$ classes in the classifier. For this we replace the sigmoid function with the *softmax* function:


$$
{s_{i}(z)=\frac{e^{z_{i}}}{\Sigma_{j=1}^{k}e^{z_{j}}}}
$$


It is by definition normalised such that the sum adds to one and is the multi-class generalisation of the sigmoid function. $s_i$ is largest for the index $i$ with the largest $z_i$. The classifier prediction will typically be the class $i$ with the largest value $s_i$.


$$
\frac{\partial s_i}{\partial z_{j}}=\frac{-e^{z_{i}}e^{z_j}+\delta_{ij}e^{z_{i}}\Sigma_{i=1}^ke^{z_i}}{\left(\Sigma_{j=1}^ke^{z_j}\right)^{2}}={s}\frac{-e^{z_j}+\delta_{ij}\Sigma_{i=1}^{\mathrm{k}}e^{z_i}}{\Sigma_{j=1}^{k}e^{z_j}}={s}\left(\delta_{ij}-s_j\right)
$$


## Multi-class loss function

The generalistaion of the cross entropy loss for multiple class is given by


$$
{J=-\sum_{i,j}y_{j}^{(i)}log(\hat y_{j}^{(i)})}
$$


where $y^{(i)}_j$ is $1$ if training sample ii is in class $j$, 0 otherwise.

This is called *one-hot encoding*. The derivative of the loss function with respect to one of the predictions is given by


$$
\frac{\partial J}{\partial \hat y_j^{(i)}}=-\frac{y_j^{(i)}}{\hat y_j^{(i)}}
$$


## Example

Using the same circle dataset that we used before, the decision function for a single-layer neural network with 10 hidden units gives the following decision boundary:

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/circles_ML.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/circles_ML.png)

Here is an example using more than two classes.

[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_3class.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_3class.png)[![img](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_ML.png)](https://miscada-ml-2324.notes.dmaitre.phyip3.dur.ac.uk/assets/lecture-5-neural-networks/neural-networks_files/iris_ML.png)
